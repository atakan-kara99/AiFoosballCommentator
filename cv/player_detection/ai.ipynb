{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "# import torch_directml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# TODO try with 720 x 1280 -> 704 x 1280 because it needs to be a multiple of 32\n",
    "img_size = (224, 384) # image width and height\n",
    "# img_size = (448, 768) # image width and height\n",
    "\n",
    "\n",
    "class FoosballDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augment=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.augment = augment\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        # Torchvision Base Transforms (Resize & Normalize)\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),  # Resize image\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1,1]\n",
    "        ])\n",
    "\n",
    "        # Albumentations Augmentations\n",
    "        self.augmentation = A.Compose([\n",
    "            A.Resize(img_size[0],img_size[1]),  # Resize\n",
    "            A.HorizontalFlip(p=0.5),  \n",
    "            A.VerticalFlip(p=0.5),  \n",
    "            A.RandomBrightnessContrast(p=0.7),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "            A.GaussNoise(p=0.5),\n",
    "            A.Rotate(limit=(-90, 90), p=0.4),\n",
    "            A.ElasticTransform(alpha=1, sigma=50, p=0.4),\n",
    "            A.CoarseDropout(num_holes_range=(1,8), hole_height_range=[30,60], hole_width_range=(30,60), p=0.5),\n",
    "            # TODO translation and scaling\n",
    "            # A.RandomScale((-0.1,0.1),cv2.INTER_LINEAR,cv2.INTER_NEAREST,p=0.5),\n",
    "            # A.Resize(180,320),  # Resize\n",
    "            A.Normalize(mean=(0.5), std=(0.5)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "\n",
    "        # Load Image & Mask\n",
    "        image = cv2.imread(img_path) \n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) \n",
    "        # image = image[:, :, 1]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) \n",
    "        \n",
    "        # Ensure binary mask (0 or 1)\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Apply Augmentation\n",
    "        if self.augment:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        else:\n",
    "            image = self.base_transform(Image.fromarray(image))  # Apply torchvision transform\n",
    "            mask = cv2.resize(mask, img_size[::-1], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # TODO use bytes instead, also in the model - performance increase?\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "        return image, mask\n",
    "\n",
    "# add the hand labeled images in different augmented ways as well as without augmentations\n",
    "hand_labeled1 = FoosballDataset(os.path.join(\"training_images\", \"images\"), os.path.join(\"training_images\", \"masks\"), augment=False)\n",
    "hand_labeled_aug1 = FoosballDataset(os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"images\"), os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"masks\"), augment=True)\n",
    "\n",
    "vid_003_aug = FoosballDataset(os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"images\"), os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"masks\"), augment=False)\n",
    "vid_003 = FoosballDataset(os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"images\"), os.path.join(\"training_images\", \"test_003_hand_cleaned\", \"masks\"), augment=True)\n",
    "\n",
    "hand_labeled = ConcatDataset([hand_labeled1,hand_labeled_aug1, vid_003, vid_003_aug])\n",
    "\n",
    "video1_dir = os.path.join(\"training_images\", \"rec-20250110-130045_hand_cleaned\")\n",
    "vid1 = FoosballDataset(os.path.join(video1_dir, \"images\"), os.path.join(video1_dir, \"masks\"), augment=True)\n",
    "\n",
    "full_dataset = ConcatDataset([hand_labeled, vid1])\n",
    "\n",
    "# Split dataset into train (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Disable augmentation for validation dataset\n",
    "val_dataset.dataset.augment = False  # Ensures val set is not augmented\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"total: {len(full_dataset)}, training: {len(train_loader.dataset)}, validation: {len(val_loader.dataset)}\")\n",
    "\n",
    "images, masks = next(iter(train_loader))\n",
    "print(f\"Training: Image batch shape: {images.shape}\")  # (batch_size, 1, H, W)\n",
    "print(f\"Training: Mask batch shape: {masks.shape}\")  # (batch_size, 1, H, W)\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    print(f\"Training: Image shape: {images.shape}\")  # Should be [batch_size, 1, H, W]\n",
    "    print(f\"Training: Mask shape: {masks.shape}\")  # Should be [batch_size, 1, H, W]\n",
    "    break\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "print(f\"Validation: Image batch shape: {images.shape}\")  # (batch_size, 1, H, W)\n",
    "print(f\"Validation: Mask batch shape: {masks.shape}\")  # (batch_size, 1, H, W)\n",
    "\n",
    "for images, masks in val_loader:\n",
    "    print(f\"Validation: Image shape: {images.shape}\")  # Should be [batch_size, 1, H, W]\n",
    "    print(f\"Validation: Mask shape: {masks.shape}\")  # Should be [batch_size, 1, H, W]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show exampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, masks = next(iter(train_loader))\n",
    "\n",
    "image = images[0]\n",
    "mask = masks[0]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.permute(1, 2, 0)) # rgb image\n",
    "# plt.imshow(image.squeeze(0),cmap=\"gray\")  # single channel images\n",
    "plt.title(\"Augmented Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask.squeeze(0), cmap=\"gray\")  # Remove channel dimension\n",
    "plt.title(\"Augmented Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowUNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1):\n",
    "        super(ShallowUNet, self).__init__()\n",
    "\n",
    "        # Downsampling path\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 4, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.pool4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.pool5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self.up3 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Upsampling path\n",
    "        self.up4 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up5 = nn.ConvTranspose2d(8, 4, kernel_size=2, stride=2)\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.Conv2d(8, 4, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.out = nn.Conv2d(4, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        enc5 = self.enc5(self.pool4(enc4))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool5(enc5))\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.dec1(torch.cat((self.up1(bottleneck), enc5), dim=1))\n",
    "        dec2 = self.dec2(torch.cat((self.up2(dec1), enc4), dim=1))\n",
    "        dec3 = self.dec3(torch.cat((self.up3(dec2), enc3), dim=1))\n",
    "        dec4 = self.dec4(torch.cat((self.up4(dec3), enc2), dim=1))\n",
    "        dec5 = self.dec5(torch.cat((self.up5(dec4), enc1), dim=1))\n",
    "\n",
    "        # Output\n",
    "        return torch.sigmoid(self.out(dec5))\n",
    "\n",
    "model = ShallowUNet(input_channels=3, output_channels=1)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################################\n",
    "# utility\n",
    "##################################################################################################################################\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "best_val_loss = float('inf')  # Keep track of best validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "# config\n",
    "##################################################################################################################################\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch_directml.device()\n",
    "\n",
    "model.to(device)\n",
    "epochs = 64\n",
    "model.train()\n",
    "\n",
    "##################################################################################################################################\n",
    "# training\n",
    "##################################################################################################################################\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)  # Average training loss\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase (No gradients)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks in val_loader:\n",
    "            val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_masks).item()\n",
    "\n",
    "    val_loss /= len(val_loader)  # Average validation loss\n",
    "    val_losses.append(val_loss)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o', color='blue')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Loss\", marker='o', color='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"{bcolors.OKGREEN}Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} (Best){bcolors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = []\n",
    "# val_losses = []\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.train()\n",
    "print(\"Starting training\")\n",
    "epochs = 32\n",
    "\n",
    "finetuning_loader = DataLoader(hand_labeled,batch_size=16,shuffle=True)\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in finetuning_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(finetuning_loader)  # Average training loss\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase (No gradients)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks in val_loader:\n",
    "            val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss += criterion(val_outputs, val_masks).item()\n",
    "\n",
    "    val_loss /= len(val_loader)  # Average validation loss\n",
    "    val_losses.append(val_loss)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o', color='blue')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Loss\", marker='o', color='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"{bcolors.OKGREEN}Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} (Best){bcolors.ENDC}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"unet_foosball.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch_directml.device()\n",
    "\n",
    "model = ShallowUNet(input_channels=3, output_channels=1)  # needs to match training config\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"unet_foosball.pth\", map_location=device))  # Load weights\n",
    "model.eval()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "img_path = os.path.join(\"training_images/unlabeled_images/\", random.choice(os.listdir(\"training_images/unlabeled_images/\")))\n",
    "# img_path = os.path.join(\"training_images/rec-20250110-130045/images/\", random.choice(os.listdir(\"training_images/rec-20250110-130045/images/\")))\n",
    "print(os.path.basename(img_path))\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "model.eval()  \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),  # Resize to match model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "rgb_image = cv2.imread(img_path)  # Read image using OpenCV\n",
    "image = rgb_image.copy()\n",
    "# image = cv2.cvtColor(rgb_image.copy(), cv2.COLOR_BGR2HSV)  # Convert to RGB\n",
    "# image = image[:, :, 1]\n",
    "rgb_image = cv2.cvtColor(rgb_image,cv2.COLOR_BGRA2RGB)\n",
    "start_time = time.time()\n",
    "image_pil = Image.fromarray(image)\n",
    "input_tensor = transform(image_pil).unsqueeze(0).to(device)  # Add batch dimension: (1, 3, W, H)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Print statistics about the output\n",
    "print(f\"Output min: {output.min().item()}, max: {output.max().item()}\")\n",
    "print(f\"Output mean: {output.mean().item()}, std: {output.std().item()}\")\n",
    "# Post-process the output\n",
    "output_mask = output.cpu().squeeze(0).squeeze(0).numpy()  # Remove batch and channel dims\n",
    "output_mask = (output_mask > 0.5).astype(np.uint8)  # Threshold to binary mask\n",
    "\n",
    "# Resize mask back to original image size\n",
    "output_mask = cv2.resize(output_mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "print(f\"time taken: {time.time()-start_time:.3f}s\")\n",
    "# Overlay mask on original image\n",
    "overlay = rgb_image.copy()\n",
    "overlay[output_mask == 1] = [255,0,0]  # Color detected areas in red\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(rgb_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(overlay)\n",
    "plt.title(\"Overlayed Segmentation\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foosball",
   "language": "python",
   "name": "foosball"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
